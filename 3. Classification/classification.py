# -*- coding: utf-8 -*-
"""Glass Classification Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGK_wJ09BhzUlM1E4ILmf0EnNeoGeA97

# **Importing  Libraries**
"""

import numpy as np
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt

"""# **Dataset Loading**"""

dataset = pd.read_csv('https://raw.githubusercontent.com/tanim913/machine-learning-python/main/1.%20Data-Preprocessing/glass.csv')

dataset.head()

#checking data type and null values
dataset.info()

#checking null values
dataset.isna().sum()

#checking Over of data distribution
dataset.describe()

"""# 3. Data Visulization"""

dataset["Type"].value_counts()

sns.countplot(x=dataset['Type'])

"""## 3.1 Glass Types Distribution Pie Chart"""

plt.style.use("seaborn")
fig, ax = plt.subplots(figsize=(10,8))

plt.pie(x=dataset["Type"].value_counts(), 
        labels=["Type 2", "Type 1", "Type 7", "Type 3", "Type 5", "Type 6"],
        shadow = True, 
        autopct="%1.2f%%", 
        )
plt.title("Glass Types Distribution Pie Chart",fontsize=15)
plt.show()

"""## 3.2 Features Visualisation"""

for column in dataset.columns[:-1]:
  sns.displot(dataset[column], color= 'g')
  plt.grid(True)
  plt.show()

"""# 4. Separating features and target column"""

# Separating Features and Label
x=dataset.drop(columns=['Type'])
y=dataset['Type']

"""# 5. Splitting dataset in train data and test data"""

# splitting dataset in train data and test data
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=4)

"""# 6.Classification Models

## 6.1 Logistic Regression
"""

# as accuracy is very less we use scalling
from sklearn.preprocessing import StandardScaler
scale=StandardScaler()
scale_xtrain=scale.fit_transform(xtrain)
scale_xtest=scale.fit_transform(xtest)

# training and fitting model
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(scale_xtrain,ytrain)
ypred1=model.predict(scale_xtest)

# Evaluation of Model
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

lg_acc = accuracy_score(ytest, ypred1)
lg_mse = mean_squared_error(ytest, ypred1)
lg_f1 = f1_score(ytest, ypred1, average='macro')
lg_precision = precision_score(ytest, ypred1, average='macro')

print('Accuracy is: %.4f' % lg_acc)
print('Mean Squared Error is: %.4f' % lg_mse)
print('F1 Score is: %.4f' % lg_f1)
print('Precision Score is:  %.4f' % lg_precision)

"""## 6.2 KNN (k-nearest neighbors)



"""

# training and fitting model
from sklearn.neighbors import KNeighborsClassifier
model=KNeighborsClassifier()
model.fit(xtrain,ytrain)
ypred2=model.predict(xtest)

# Evaluation of Model
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

knn_acc = accuracy_score(ytest,ypred2)
knn_mse = mean_squared_error(ytest, ypred2)
knn_f1 = f1_score(ytest, ypred2, average='macro')
knn_precision = precision_score(ytest, ypred2, average='macro')


print('Accuracy is: %.4f' % knn_acc)
print('Mean Squared Error is: %.4f' % knn_mse)
print('F1 Score is: %.4f' % knn_f1)
print('Precision Score is:  %.4f' % knn_precision)

"""## 6.3 SVC Support Vector Classifier

"""

# training and fitting model
from sklearn.svm import SVC
model=SVC(kernel="linear")
model.fit(scale_xtrain,ytrain)
ypred3=model.predict(scale_xtest)

# Evaluation of Model
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

svc_acc = accuracy_score(ytest,ypred3)
svc_mse = mean_squared_error(ytest, ypred3)
svc_f1 = f1_score(ytest, ypred3, average='macro')
svc_precision = precision_score(ytest, ypred3, average='macro')


print('Accuracy is: %.4f' % svc_acc)
print('Mean Squared Error is: %.4f' % svc_mse)
print('F1 Score is: %.4f' % svc_f1)
print('Precision Score is:  %.4f' % svc_precision)

"""## 6.4 Naive Byes

"""

# training and fitting model
from sklearn.naive_bayes import GaussianNB
model=GaussianNB()
model.fit(xtrain,ytrain)
ypred4=model.predict(xtest)

# Evaluation of Model
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

GNB_acc = accuracy_score(ytest,ypred4)
GNB_mse = mean_squared_error(ytest, ypred4)
GNB_f1 = f1_score(ytest, ypred4, average='macro')
GNB_precision = precision_score(ytest, ypred4, average='macro')


print('Accuracy is: %.4f' % GNB_acc)
print('Mean Squared Error is: %.4f' % GNB_mse)
print('F1 Score is: %.4f' % GNB_f1)
print('Precision Score is:  %.4f' % GNB_precision)

"""## 6.5 Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
model=DecisionTreeClassifier()
model.fit(xtrain,ytrain)
ypred5=model.predict(xtest)

from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

Dt_acc = accuracy_score(ytest,ypred5)
Dt_mse = mean_squared_error(ytest, ypred5)
Dt_f1 = f1_score(ytest, ypred5, average='macro')
Dt_precision = precision_score(ytest, ypred5, average='macro', zero_division=1)


print('Accuracy is: %.4f' % Dt_acc)
print('Mean Squared Error is: %.4f' % Dt_mse)
print('F1 Score is: %.4f' % Dt_f1)
print('Precision Score is:  %.4f' % Dt_precision)

"""## 6.6 Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier(n_estimators=20)
model.fit(xtrain,ytrain)
ypred6=model.predict(xtest)

from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, precision_score

RFC_acc = accuracy_score(ytest,ypred6)
RFC_mse = mean_squared_error(ytest, ypred6)
RFC_f1 = f1_score(ytest, ypred6, average='macro')
RFC_precision = precision_score(ytest, ypred6, average='macro', zero_division=1)


print('Accuracy is: %.4f' % RFC_acc)
print('Mean Squared Error is: %.4f' % RFC_mse)
print('F1 Score is: %.4f' % RFC_f1)
print('Precision Score is:  %.4f' % RFC_precision)

"""#7. Comparing the accuracy of different models"""

models=[("LogisticRegression",lg_acc),
        ("KNeighborsClassifier",knn_acc),
        ("SVM",svc_acc),
        ("GuessinNB",GNB_acc),
        ("DecisionTree",Dt_acc),
        ("RanodmForest",RFC_acc),
        
]

predict_percent = pd.DataFrame(data = models, columns=['Model', "Accuracy"])
predict_percent

# plogttin bargraph of r2score of each model
f, axe = plt.subplots(1,1, figsize=(15,10))
predict_percent.sort_values(by=['Accuracy'], ascending=False, inplace=True)

sns.barplot(x='Accuracy', y='Model', data = predict_percent, ax = axe)
axe.set_xticks(np.arange(0, 1.1, 0.1))
plt.show()